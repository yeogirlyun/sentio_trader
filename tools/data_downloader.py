import os
import argparse
import requests
import pandas as pd
import pandas_market_calendars as mcal
import struct
from datetime import datetime
from pathlib import Path

# --- Constants ---
# Define the Regular Trading Hours for NYSE in New York time.
RTH_START = "09:30"
RTH_END = "16:00"
NY_TIMEZONE = "America/New_York"
POLYGON_API_BASE = "https://api.polygon.io"

def fetch_aggs_all(symbol, start_date, end_date, api_key, timespan="minute", multiplier=1):
    """
    Fetches all aggregate bars for a symbol within a date range from Polygon.io.
    Handles API pagination automatically.
    """
    print(f"Fetching '{symbol}' data from {start_date} to {end_date}...")
    url = (
        f"{POLYGON_API_BASE}/v2/aggs/ticker/{symbol}/range/{multiplier}/{timespan}/"
        f"{start_date}/{end_date}?adjusted=true&sort=asc&limit=50000"
    )
    
    headers = {"Authorization": f"Bearer {api_key}"}
    all_bars = []
    
    while url:
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            data = response.json()

            if "results" in data:
                all_bars.extend(data["results"])
                print(f" -> Fetched {len(data['results'])} bars...", end="\r")

            url = data.get("next_url")

        except requests.exceptions.RequestException as e:
            print(f"\nAPI Error fetching data for {symbol}: {e}")
            return None
        except Exception as e:
            print(f"\nAn unexpected error occurred: {e}")
            return None
            
    print(f"\n -> Total bars fetched for {symbol}: {len(all_bars)}")
    if not all_bars:
        return None
        
    # Convert to DataFrame for easier processing
    df = pd.DataFrame(all_bars)
    df.rename(columns={
        't': 'timestamp_utc_ms',
        'o': 'open',
        'h': 'high',
        'l': 'low',
        'c': 'close',
        'v': 'volume'
    }, inplace=True)
    return df

def filter_and_prepare_data(df):
    """
    Filters a DataFrame of market data for RTH (Regular Trading Hours)
    and removes US market holidays.
    """
    if df is None or df.empty:
        return None

    print("Filtering data for RTH and US market holidays...")
    
    # 1. Convert UTC millisecond timestamp to a timezone-aware DatetimeIndex
    df['timestamp_utc_ms'] = pd.to_datetime(df['timestamp_utc_ms'], unit='ms', utc=True)
    df.set_index('timestamp_utc_ms', inplace=True)
    
    # 2. Convert the index to New York time to perform RTH and holiday checks
    df.index = df.index.tz_convert(NY_TIMEZONE)
    
    # 3. Filter for Regular Trading Hours
    df = df.between_time(RTH_START, RTH_END)

    # 4. Filter out US market holidays
    nyse = mcal.get_calendar('NYSE')
    holidays = nyse.holidays().holidays # Get a list of holiday dates
    df = df[~df.index.normalize().isin(holidays)]
    
    print(f" -> {len(df)} bars remaining after filtering.")
    
    # 5. Add the specific columns required by the C++ backtester
    df['ts_utc'] = df.index.strftime('%Y-%m-%dT%H:%M:%S%z').str.replace(r'([+-])(\d{2})(\d{2})', r'\1\2:\3', regex=True)
    df['ts_nyt_epoch'] = df.index.astype('int64') // 10**9
    
    return df

def save_to_bin(df, path):
    """
    Saves the DataFrame to a custom binary format compatible with the C++ backtester.
    Format:
    - uint64_t: Number of bars
    - For each bar:
      - uint32_t: Length of ts_utc string
      - char[]: ts_utc string data
      - int64_t: ts_nyt_epoch
      - double: open, high, low, close
      - uint64_t: volume
    """
    print(f"Saving to binary format at {path}...")
    try:
        with open(path, 'wb') as f:
            # Write total number of bars
            num_bars = len(df)
            f.write(struct.pack('<Q', num_bars))

            # **FIXED**: The struct format string now correctly includes six format
            # specifiers to match the six arguments passed to pack().
            # q: int64_t (ts_nyt_epoch)
            # d: double (open)
            # d: double (high)
            # d: double (low)
            # d: double (close)
            # Q: uint64_t (volume)
            bar_struct = struct.Struct('<qddddQ')

            for row in df.itertuples():
                # Handle the variable-length string part
                ts_utc_bytes = row.ts_utc.encode('utf-8')
                f.write(struct.pack('<I', len(ts_utc_bytes)))
                f.write(ts_utc_bytes)
                
                # Pack and write the fixed-size data
                packed_data = bar_struct.pack(
                    row.ts_nyt_epoch,
                    row.open,
                    row.high,
                    row.low,
                    row.close,
                    int(row.volume) # C++ expects uint64_t, so we cast to int
                )
                f.write(packed_data)
        print(" -> Binary file saved successfully.")
    except Exception as e:
        print(f"Error saving binary file: {e}")

def main():
    parser = argparse.ArgumentParser(description="Polygon.io Data Downloader and Processor")
    parser.add_argument('symbols', nargs='+', help="One or more stock symbols (e.g., QQQ TQQQ SQQQ)")
    parser.add_argument('--start', required=True, help="Start date in YYYY-MM-DD format")
    parser.add_argument('--end', required=True, help="End date in YYYY-MM-DD format")
    parser.add_argument('--outdir', default='data', help="Output directory for CSV and BIN files")
    parser.add_argument('--timespan', default='minute', choices=['minute', 'hour', 'day'], help="Timespan of bars")
    parser.add_argument('--multiplier', default=1, type=int, help="Multiplier for the timespan")
    
    args = parser.parse_args()
    
    # Get API key from environment variable for security
    api_key = os.getenv('POLYGON_API_KEY')
    if not api_key:
        print("Error: POLYGON_API_KEY environment variable not set.")
        return
        
    # Create output directory if it doesn't exist
    output_dir = Path(args.outdir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for symbol in args.symbols:
        print("-" * 50)
        # 1. Fetch data
        df_raw = fetch_aggs_all(symbol, args.start, args.end, api_key, args.timespan, args.multiplier)
        
        if df_raw is None or df_raw.empty:
            print(f"No data fetched for {symbol}. Skipping.")
            continue
            
        # 2. Filter and prepare data
        df_clean = filter_and_prepare_data(df_raw)
        
        if df_clean is None or df_clean.empty:
            print(f"No data remaining for {symbol} after filtering. Skipping.")
            continue
        
        # 3. Define output paths
        file_prefix = f"{symbol.upper()}_RTH_NH"
        csv_path = output_dir / f"{file_prefix}.csv"
        bin_path = output_dir / f"{file_prefix}.bin"
        
        # 4. Save to CSV for inspection
        print(f"Saving to CSV format at {csv_path}...")
        # Select and order columns to match C++ struct for clarity
        csv_columns = ['ts_utc', 'ts_nyt_epoch', 'open', 'high', 'low', 'close', 'volume']
        df_clean[csv_columns].to_csv(csv_path, index=False)
        print(" -> CSV file saved successfully.")
        
        # 5. Save to C++ compatible binary format
        save_to_bin(df_clean, bin_path)

    print("-" * 50)
    print("Data download and processing complete.")

if __name__ == "__main__":
    main()
